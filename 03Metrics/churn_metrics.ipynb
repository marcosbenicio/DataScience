{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printest(args, value):\n",
    "    return print( \"{} : \\n {} \\n\".format(args, value) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - prepare data and train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn prediction is about identifying customers who are likely to cancel their contracts soon. If the company can do that, it can offer discounts on these services in an effort to keep the users. Here we use the dataset of churn prediction for a telecom company.\n",
    "\n",
    "- A value of 0 indicates that the customer did not churn (they stayed with the service).\n",
    "- A value of 1 indicates that the customer did churn (they left the service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Convert 'TotalCharges' to numeric, replace non-numeric with NaN\n",
    "df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
    "# Fill NaN values with zero\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "\n",
    "# lowering columns name and replace spaces by _\n",
    "df.columns  = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# boolean mask to select columns with object type (string)\n",
    "obj_mask = df.dtypes == 'object' \n",
    "obj_columns = list(df.dtypes[obj_mask].index)\n",
    "\n",
    "# lowering rows strings and replace spaces by _\n",
    "for col in obj_columns:\n",
    "    df[col] = df[col].str.lower().str.replace(' ', '_')\n",
    "\n",
    "# turn 'no' into 0 and 'yes' into 1\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, test, validation\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.33, random_state=11)\n",
    "\n",
    "# save target values\n",
    "y_train = df_train['churn'].values\n",
    "y_val = df_val['churn'].values\n",
    "\n",
    "# take out the target values from the dataframe\n",
    "del df_train['churn']\n",
    "del df_val['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical : \n",
      " ['gender', 'partner', 'dependents', 'phoneservice', 'multiplelines', 'internetservice', 'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport', 'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling', 'paymentmethod', 'seniorcitizen'] \n",
      "\n",
      "numerical : \n",
      " ['tenure', 'monthlycharges', 'totalcharges'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns except 'customerid'\n",
    "categorical_mask = df_train.dtypes == 'object'\n",
    "categorical = list(df_train.dtypes[categorical_mask].index)\n",
    "categorical.remove('customerid') \n",
    "\n",
    "# Manually add 'seniorcitizen' because it's an int boolean (0 or 1)\n",
    "categorical.append('seniorcitizen')\n",
    "printest('categorical', categorical)\n",
    "\n",
    "# All numerical columns except 'seniorcitizen' because it's an int boolean\n",
    "numerical_mask = df_train.dtypes != 'object'\n",
    "numerical = list(df_train.dtypes[numerical_mask].index)\n",
    "numerical.remove('seniorcitizen')\n",
    "printest('numerical', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=1, solver='liblinear')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform dataframe to dict\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "# One-hot encoding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(train_dict)\n",
    "X_train = dv.transform(train_dict)\n",
    "\n",
    "# Model train for all relevant features\n",
    "model = LogisticRegression(solver='liblinear', random_state=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob Not churn| Prob Churn : \n",
      " [[0.76508893 0.23491107]\n",
      " [0.7311339  0.2688661 ]\n",
      " [0.6805482  0.3194518 ]\n",
      " ...\n",
      " [0.94274725 0.05725275]\n",
      " [0.38476961 0.61523039]\n",
      " [0.93872737 0.06127263]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate the model\n",
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "y_pred = model.predict_proba(X_val)\n",
    "printest('Prob Not churn| Prob Churn',y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob Not churn| Prob Churn : \n",
      " [[0.76508893 0.23491107]\n",
      " [0.7311339  0.2688661 ]\n",
      " [0.6805482  0.3194518 ]\n",
      " ...\n",
      " [0.94274725 0.05725275]\n",
      " [0.38476961 0.61523039]\n",
      " [0.93872737 0.06127263]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training the model for a subset of features\n",
    "small_subset = ['contract', 'tenure', 'totalcharges']\n",
    "train_dict_small = df_train[small_subset].to_dict(orient='records')\n",
    "dv_small = DictVectorizer(sparse=False)\n",
    "dv_small.fit(train_dict_small)\n",
    "\n",
    "X_small_train = dv_small.transform(train_dict_small)\n",
    "\n",
    "model_small = LogisticRegression(solver='liblinear', random_state=1)\n",
    "model_small.fit(X_small_train, y_train)\n",
    "\n",
    "val_dict_small = df_val[small_subset].to_dict(orient='records')\n",
    "X_small_val = dv_small.transform(val_dict_small)\n",
    "\n",
    "y_pred_small = model_small.predict_proba(X_small_val)\n",
    "printest('Prob Not churn| Prob Churn',y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Evaluation metrics serve as a critical tool for quantifying the performance of a model. They function by assessing the model's predictions against the actual observed values. This comparison yields a measurement that indicates the model's predictive accuracy. Therefore, evaluation metrics provide an essential insight into the model's proficiency and its ability to generalize to unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a binary classification model is determined by the proportion of predictions it correctly makes relative to the total number of predictions. In mathematical terms, this can be represented as:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{correct}}{\\text{total}} = \\frac{1491}{1860} = 80\\%$$\n",
    "\n",
    "This metric provides valuable insight into the model's performance. By tallying the instances where our model's predictions align with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Model Accuracy : \n",
      " 0.8016129032258065 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy explicitly\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "# probability threshold\n",
    "churn = y_pred >= 0.5\n",
    "correct = (y_val == churn).sum()\n",
    "total = len(y_val)\n",
    "accuracy = correct/total\n",
    "printest('Full Model Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Model Accuracy : \n",
      " 0.7672043010752688 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy explicitly\n",
    "y_pred_small = model_small.predict_proba(X_small_val)[:, 1]\n",
    "# probability threshold\n",
    "churn = y_pred_small >= 0.5\n",
    "correct = (y_val == churn).sum()\n",
    "total = len(y_val)\n",
    "accuracy = correct/total\n",
    "printest('Small Model Accuracy', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the optimal threshold, we can iteratively calculate the accuracy for a range of potential threshold values. This process allows us to assess the performance of our model at various decision boundaries, thereby enabling us to select the threshold that yields the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.261290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.501075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.594624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>0.640323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>0.689785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.729570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.754839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>0.767204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>0.781720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>0.795161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.801613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>0.789785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>0.788172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>0.773656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.752151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>0.738710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.738710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.738710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.738710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy\n",
       "threshold          \n",
       "0.00       0.261290\n",
       "0.05       0.501075\n",
       "0.10       0.594624\n",
       "0.15       0.640323\n",
       "0.20       0.689785\n",
       "0.25       0.729570\n",
       "0.30       0.754839\n",
       "0.35       0.767204\n",
       "0.40       0.781720\n",
       "0.45       0.795161\n",
       "0.50       0.801613\n",
       "0.55       0.790323\n",
       "0.60       0.789785\n",
       "0.65       0.788172\n",
       "0.70       0.773656\n",
       "0.75       0.752151\n",
       "0.80       0.741935\n",
       "0.85       0.738710\n",
       "0.90       0.738710\n",
       "0.95       0.738710\n",
       "1.00       0.738710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 21)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "    acc = accuracy_score(y_val, y_pred >= t)\n",
    "    accuracies.append(acc)\n",
    "    #print('%0.2f %0.3f' % (t, acc))\n",
    "\n",
    "# Create a dictionary\n",
    "data = {'threshold': thresholds, 'accuracy': accuracies}\n",
    "\n",
    "# Create DataFrame\n",
    "df_acc = pd.DataFrame(data)\n",
    "\n",
    "# Print DataFrame\n",
    "display(df_acc.set_index('threshold'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, using the threshold of 0.5 gives us the best accuracy. Typically, 0.5 is a good\n",
    "threshold value to start with. To make it more visual, we can use Matplotlib to create a plot that shows how accuracy changes depending on the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(df_acc['threshold'].values, df_acc['accuracy'].values, color='black')\n",
    "\n",
    "plt.title('Threshold vs Accuracy')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xticks(np.linspace(0, 1, 11))\n",
    "\n",
    "# plt.savefig('04_threshold_accuracy.svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline\n",
    "\n",
    "Establishing a baseline is an important step in evaluating the performance of our predictive models. For this purpose, we often use a dummy model — a simple construct which consistently predicts the same outcome, regardless of the input features. In our case, it consistently outputs 'False'. \n",
    "\n",
    "By comparing the performance of our complex models against this baseline, we gain a clear understanding of their incremental improvement in predictive accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Model Accuracy : \n",
      " 0.7672043010752688 \n",
      "\n",
      "Small Model Accuracy : \n",
      " 0.7672043010752688 \n",
      "\n",
      "Baseline accuracy : \n",
      " 0.7387096774193549 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dummy model that outputs only False\n",
    "size_val = len(y_val)\n",
    "baseline = np.repeat(False, size_val)\n",
    "\n",
    "\n",
    "printest('Full Model Accuracy', accuracy)\n",
    "printest('Small Model Accuracy', accuracy)\n",
    "printest('Baseline accuracy', accuracy_score(baseline, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the performance of our models, we notice that the smaller model surpasses the naive baseline by a marginal 2%, while the larger model achieves a slightly better improvement of 6%. According to accuracy metrics, our model only offers a marginal improvement over a rudimentary model that categorizes all customers as non-churning and makes no effort to retain them. \n",
    "\n",
    "This phenomenon often arises in scenarios where there is class imbalance, a condition wherein one class outnumbers the other. This imbalance is indeed apparent in our dataset, where 74% of customers didn't churn while only 26% did. For this we need another metric that validate the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Measures\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for multiple classes**\n",
    "\n",
    "The accuracy is a global measure in that it does not explicitly consider the classes that contribute to the error. A more detailed understanding can be achieved by noting the agreement and disagreement for specific classes between the actual and predicted labels in the validation or test set. \n",
    "\n",
    "Consider a typical dataset for the validation points with multiple classes,\n",
    "$$\\mathbf{D} =\n",
    "\\left( \\begin{array}{c|cccc|c}\n",
    "~    &X_{0}&X_{1}&\\cdots & X_{d}  & Y\\\\\n",
    "\\hline\n",
    "\\mathbf{x}_{1} &1& x_{11}& \\cdots&x_{1d}&y_1 \\\\\n",
    "\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{x}_{n}&1&x_{n1}&\\cdots&x_{nd}&y_n\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "The target values, denoted as $\\mathbf{Y} = (y_1, \\cdots, y_i, \\cdots, y_n)$, are not binary; instead, each $y_i \\in \\{c_1, \\cdots, c_k\\}$ represents one of $k$ class labels. Here, $\\mathcal{D}= \\{\\mathbf{D}_1,\\mathbf{D}_2, \\cdots,\\mathbf{D}_k\\}$ denotes the division of validation data according to their true class labels $y$.\n",
    "\n",
    "The $j^{th}$ split for the target values $y_i$ on the validation set is defined as:\n",
    "\n",
    "$$\\mathbf{D}_j = \\{\\mathbf{x}_i | y_i = c_j\\}$$\n",
    "\n",
    "with $n_i = |\\mathbf{D}_i|$ as the count of the true class $c_i$ within the validation set.\n",
    "\n",
    "Another division is conducted based on the predicted values from the validation set, forming $\\mathcal{R} = \\{\\mathbf{R}_1, \\mathbf{R}_2, \\cdots, \\mathbf{R}_k\\}$. This denotes that the $j^{th}$ partition for the predicted target value $\\hat{y}_i$ on the validation set is:\n",
    "\n",
    "$$\\mathbf{R}_j = \\{\\mathbf{x}_i | \\hat{y}_i = c_j\\}$$\n",
    "\n",
    "and $m_i = |\\mathbf{R}_i|$ represents the quantity of the predicted class $c_j$.\n",
    "\n",
    "The divisions $\\mathcal{R}$ and $\\mathcal{D}$ yield a $k \\times k$ matrix $\\mathbf{N}$, better known as a confusion matrix. This matrix illustrates the intersections of each partition from the sets $\\mathcal{R}$ and $\\mathcal{D}$:\n",
    "\n",
    "$$\\mathbf{N}(i,j) = n_{ij} = |\\mathbf{R}_i \\cap \\mathbf{D}_j |  = |\\{\\mathbf{x}_a \\in \\mathbf{D}| \\hat{y}_a = c_j ~~\\text{and} ~~ y_a = c_i \\}|$$\n",
    "\n",
    "For $1\\leq i$, $j \\leq k$, $n_{ij}$ represents the count of instances with a predicted class of $c_i$ and an actual label of $c_j$. These matrix values account for the total instances in each intersection. The matrix is presented as:\n",
    "\n",
    "$$\\mathbf{N} =\n",
    "\\left( \\begin{array}{c|cccc}\n",
    "~  y_{i} |~~\\hat{y}_i  &c_{1}&c_{2}&\\cdots & c_{k} \\\\\n",
    "\\hline\n",
    "{c}_{1} &n_{11}& n_{12}& \\cdots&n_{1k} \\\\\n",
    "{c}_{2} &n_{21}& n_{22}& \\cdots&n_{2k} \\\\\n",
    "\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\\\\n",
    "c_{k}&n_{k1}&n_{k2}&\\cdots&n_{kk}\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "The diagonal elements, $n_{ii} = |\\mathbf{R}_i \\cap \\mathbf{D}_i|$ (for $1\\leq i \\leq k$), denote instances where the classifier correctly identifies the true label $c_i$. The size of $\\mathbf{R}_i$ (the set of instances predicted as class $c_i$) and $\\mathbf{D}i$ (the set of instances that are truly class $c_i$) and their relationship with $n_{ii}$ can provide further insights:\n",
    "\n",
    "- If the size of $\\mathbf{R}_i$ is equal to the size of $\\mathbf{D}_i$, and $n_{ii}$ equals to the size of these sets, then the classifier has made perfect predictions for class $c_i$.\n",
    "\n",
    "- If the size of $\\mathbf{R}_i$ is larger than the size of $\\mathbf{D}_i$, it means that the classifier has predicted more instances as class $c_i$ than there actually are.\n",
    "\n",
    "- If the size of $\\mathbf{R}_i$ is smaller than the size of $\\mathbf{D}_i$, it means that the classifier has predicted fewer instances as class $c_i$ than there actually are.\n",
    "\n",
    "The off-diagonal elements, $n_{ij} = |\\mathbf{R}_i \\cap \\mathbf{D}_j|$ where $i \\neq j$, represent instances where the classifier's predictions and the true labels diverge. If $n_{ij}$ is non-zero, it signifies that there are instances where the classifier predicted class $c_i$ while the true class was $c_j$, indicating a misclassification error.\n",
    "\n",
    "- If the size of $\\mathbf{R}_i$ is larger than the size of $\\mathbf{D}_j$, it suggests that the classifier has over-predicted instances as class $c_i$ when they are actually class $c_j$. \n",
    "\n",
    "- If the size of $\\mathbf{R}_i$ is smaller than the size of $\\mathbf{D}_j$, it implies that the classifier has under-predicted instances as class $c_i$ when they are actually class $c_j$. \n",
    "\n",
    "To measure the quality of the classifier, we can calculate rates with respect to the sizes of each subset and their intersection:\n",
    "\n",
    "**Accuracy/Precision**\n",
    "\n",
    "The precision for a specific class $c_i$ is defined as the proportion of accurate predictions out of all points predicted to belong to class $c_i$:\n",
    "\n",
    "$$\\text{acc}_i = \\text{prec}_i= \\frac{|\\mathbf{R}_i \\cap \\mathbf{D}_i | }{|\\mathbf{R}_i|}  = \\frac{n_{ii}}{m_i}$$\n",
    "\n",
    "Here, $m_i$ is the total number of predictions for class $c_i$, and $n_{ii}$ represents instances where the classifier correctly identifies the true label $c_i$. A high precision for class $c_i$ indicates a more effective classifier.\n",
    "\n",
    "**Covarage/Recall**\n",
    "\n",
    "Recall, also known as coverage, for class $c_i$ is determined by the ratio of correct predictions to all points actually in class $c_i$:\n",
    "\n",
    "$$\\text{coverage}_i= \\text{recall}_i = \\frac{|\\mathbf{R}_i \\cap \\mathbf{D}_i | }{|\\mathbf{D}_i|} =  \\frac{n_{ii}}{n_i}$$\n",
    "\n",
    "Here, $n_i$ denotes the number of instances in class $c_i$. A high recall for class $c_i$ implies a more capable classifier.\n",
    "\n",
    "**F-Measures**\n",
    "\n",
    "Precision and recall often present a trade-off when assessing classifier performance. The F-measure seeks to balance these values by calculating their harmonic mean for class $c_i$:\n",
    "\n",
    "- If $\\text{recall}_i = 1$, the classifier predicts all validation points as belonging to class $c_i$. However, this will usually result in a low precision $\\text{prec}_i$.\n",
    "\n",
    "- If $\\text{prec}_i$ is very high because the classifier predicts only a few points as $c_i$, the recall $\\text{recall}_i$ is likely to be low.\n",
    "\n",
    "The F-measure seeks to harmonize the precision and recall values by calculating their harmonic mean for class $c_i$:\n",
    "\n",
    "$$F_i = \\frac{2}{\\frac{1}{\\text{prec}_i} + \\frac{1}{\\text{recall}_i}} = \\frac{2\\text{prec}_i \\text{recall}_i}{\\text{prec}_i + \\text{recall}_i} = \\frac{2n_{ii}}{n_i + m_i}$$\n",
    "\n",
    "The higher the $F_i$ value, the more effective the classifier for the specific class.\n",
    "\n",
    "The overall F- measure for the classifier is the mean of the class-specific values:\n",
    "\n",
    "$$F = \\frac{1}{k}\\sum^r_{i=1}F_i$$\n",
    "\n",
    "For perfect classifier, the maximum value of F measure is 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix for Binary classes**\n",
    "\n",
    "After considering a general case of multiple classes, we now turn our focus to the specific instance of binary classification. In such a scenario, where $k = 2$, we designate $c_1 = c_+$ as the positive class and $c_2 = c_{~-}$ as the negative class. The target values are represented as $\\mathbf{Y} = (y_{~+}, y_{~-})$. Consequently, the sets of true values are denoted as $\\mathcal{D} = {\\mathbf{D}_{~-}, \\mathbf{D}_{~+}}$, and the predicted values are indicated as $\\mathcal{R} = {\\mathbf{R}_{~-}, \\mathbf{R}_{~+}}$. The divisions $\\mathcal{R}$ and $\\mathcal{D}$ form a $2 \\times 2$ matrix, as presented below:\n",
    "\n",
    "$$\\mathbf{N} =\n",
    "\\left( \\begin{array}{c|cc}\n",
    "~  y_{i} |~~\\hat{y}_i  &c_{~+}& c_{~-} \\\\\n",
    "\\hline\n",
    "{c}_{~+} &n_{11}& n_{12}&  \\\\\n",
    "{c}_{~-} &n_{21}& n_{22}&  \\\\\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "Each entry of the matrix corresponds to the size of the intersection of the subsets $n_{ij} = |\\mathbf{R}_i \\cap \\mathbf{D}_j |$ for $i,j = \\{+,-\\}$. In the context of binary classification, each entry of the matrix is given a specific name:\n",
    "\n",
    "- **True Positives (TP)**: the number of points that the classifier correctly predicts as positive:\n",
    "  \n",
    "  $$\\text{TP} = n_{11} = |\\mathbf{R}_{~+} \\cap \\mathbf{D}_{~+} | = |\\{\\mathbf{x}_i|\\hat{y}_i = y_i = c_{~+}\\}|$$\n",
    "\n",
    "\n",
    "- **False Positives (FP)**:The number of points the classifier predicts to be positive, wich in fact belong to the negative class:\n",
    "\n",
    "  $$\\text{FP} = n_{12} = |\\mathbf{R}_{~+} \\cap \\mathbf{D}_{~-} | = |\\{\\mathbf{x}_i|\\hat{y}_i = c_{~+}~~\\text{and}~~ y_i = c_{~-}\\}|$$\n",
    " \n",
    "- **False Negatives (FN)**: The number of points the classifier predicts to be in the negative class, wich in fact belongs to the positive class:\n",
    "\n",
    "  $$\\text{FN} = n_{21} = |\\mathbf{R}_{~-} \\cap \\mathbf{D}_{~+} | = |\\{\\mathbf{x}_i|\\hat{y}_i = c_{~-}~~\\text{and}~~ y_i = c_{~+}\\}|$$\n",
    "\n",
    "- **True Negatives (TN)**: The number of points that the classifier correctly predicts as negative:\n",
    "\n",
    "  $$\\text{TN} = n_{22} = |\\mathbf{R}_{~-} \\cap \\mathbf{D}_{~-} | = |\\{\\mathbf{x}_i|\\hat{y}_i = y_i = c_{~-}\\}|$$\n",
    "\n",
    "\n",
    "**Global Measures**\n",
    "\n",
    "Global measures of the classifier performance\n",
    "\n",
    "- **Error rate**: The proportion of incorrect predictions:\n",
    "\n",
    "$$\\text{Error Rate} = \\frac{\\text{FP} + \\text{FN}}{n}$$\n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{n}$$\n",
    "\n",
    "where $n$ is the total points in the validation dataset.\n",
    "\n",
    "**Class Specific Measures**\n",
    "\n",
    "- **Precision**: The ratio of correct predictions to all points predicted to belong to the positive or negative class\n",
    "\n",
    " $$\\text{prec}_i = \\frac{n_{ii}}{m_i} = \\left\\{ \\begin{array}{ll} \n",
    "\\frac{n_{11}}{m_{~+}} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}} \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{n_{22}}{m_{~-}} = \\frac{\\text{TN}}{\\text{TN}+\\text{FN}}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    " where $m_i = |\\mathbf{R}_i|$ is the number of points predicted having class $c_i$\n",
    "\n",
    "- **Recall**: The proportion of correct predictions out of all points in the positive or negative class.\n",
    "  \n",
    "  - **True Positive Rate (Sensitivity)**: \n",
    "\n",
    "  $$\\text{recall}_{~+} = \\text{sensitivity}= \\frac{n_{11}}{n_{~+}} =\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}  $$\n",
    "\n",
    "  where $n_{~+} = |\\mathbf{D}_{~+}|$  is the size of the positive class.\n",
    "\n",
    "  - **True Negative Rate (Specificity)**:\n",
    "\n",
    "  $$\\text{recall}_{~-} = \\text{specificity} = \\frac{n_{22}}{n_{~-}} = \\frac{\\text{TN}}{n_{~-}} = \\frac{\\text{TN}}{\\text{FP}+\\text{TN}} $$\n",
    "\n",
    "   where $n_{~-} = |\\mathbf{D}_{~-}|$ is the size of the negative class.\n",
    "\n",
    "- **$1 -$ Recall** : The ratio of incorrect predictions to all points in the positive or negative class.\n",
    "\n",
    "  - **False Negative Rate (FNR)**:\n",
    "    \n",
    "    $$1- \\text{sensitivity} = \\frac{n_{21}}{n_{~+}} =  \\frac{\\text{FN}}{\\text{TP}+\\text{FN}}$$\n",
    "    \n",
    "  - **False Positive Rate (FPR)**:\n",
    "\n",
    "    $$1- \\text{specificity} = \\frac{ n_{12} } {n_{~-}} =  \\frac{ \\text{FP} }{ \\text{FP}+\\text{TN} }$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0743c798041383927c03f3bae5eca888e94e4777f716cdd511cdff96fbdedc02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
